---
title: "Kinyarwanda Text-to-Speech for Tekana"
subtitle: "The full story — from zero to a working voice, told step by step"
author: "Didier Ngamije — Data Scientist"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
    number_sections: true
    css: report_style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

<div class="plain-box">
**How to read this document.** Every technical term is followed immediately by a plain-English explanation in a box like this one. You can read straight through as a story, or jump to any section using the menu on the left. All numbers, formulas, and decisions are included — but nothing is left unexplained.
</div>

---

# Where It All Started

## What I was handed

I was given a brief, a dataset, and a set of hard rules. The brief was simple: *Tekana answers health calls from young Rwandans. It can hear the caller and write a reply. But it cannot speak. Build the voice.*

The dataset was approximately **14 hours of Kinyarwanda speech** — recordings from three different speakers, already cleaned and split into training, validation, and test sets. Each audio clip came with a written Kinyarwanda transcription and a label saying which speaker recorded it.

The hard rules were:

| Rule | What it means |
|---|---|
| Inference latency under **800 ms** | The voice must start playing within 0.8 seconds of getting the text |
| Model size ideally under **200 MB** (hard limit **1 GB**) | The model file must be small enough to live on the server |
| **No external API** for user audio | No sending speech data to Google, Amazon, or anyone else |
| **CPU only** | No powerful GPU available for training |

<div class="plain-box">
**In plain English:** I needed to build a voice for a phone-line AI that speaks Kinyarwanda. It had to respond fast, be small enough to run on a regular server, keep the caller's data private, and be trained without expensive hardware.
</div>

---

# Choosing the Right Model

## The four options I considered

Before writing a single line of training code, I spent time evaluating which approach to take. I looked at four options seriously.

### Option 1 — Piper TTS

Piper is a lightweight, fast TTS system that converts models to ONNX format (a universal model format that runs efficiently on any hardware). The problem: **no Kinyarwanda checkpoint existed**. I would have had to train a voice from scratch, which requires hundreds of hours of data and weeks of compute time. Ruled out.

<div class="plain-box">
**In plain English:** Piper is fast and small, but it had never learned Kinyarwanda. Teaching it from scratch would have taken too long with the hardware I had.
</div>

### Option 2 — Coqui TTS

Coqui is a flexible, open-source TTS framework. Same problem: **no Kinyarwanda model available**. Starting from scratch on CPU was not feasible within the project timeline.

### Option 3 — Train VITS from scratch

VITS is the underlying architecture used by most modern TTS systems (I explain what VITS is in the next section). Training it completely from zero requires **hundreds of hours of speech data** and **GPU infrastructure**. With 14 hours and CPU only, this was not a realistic option.

### Option 4 — facebook/mms-tts-kin ✓ (what I chose)

Meta AI Research's **Massively Multilingual Speech (MMS)** project trained TTS models for over 1,100 languages, including Kinyarwanda. The model is `facebook/mms-tts-kin`, available for free on Hugging Face. It already knows how to speak Kinyarwanda. My job was to **fine-tune** it — to take this existing voice and adapt it to the specific Kinyarwanda in the Tekana dataset.

<div class="plain-box">
**In plain English:** Instead of teaching a model to speak Kinyarwanda from zero (which would take months), I found a model that already spoke Kinyarwanda and just needed some extra training on our specific data. Think of it like hiring a native Kinyarwanda speaker and coaching them on the particular style and vocabulary they need for this job — rather than teaching someone the language from scratch.
</div>

**Why this was the right choice:**

- It already had a Kinyarwanda phonetic and linguistic foundation
- It integrates directly with the Hugging Face `transformers` library — standard, reproducible, open-source
- Its file size (~250 MB) is within the hard limit
- Fine-tuning it on our data was feasible within the CPU time available

## What VITS actually is

VITS stands for **Variational Inference with adversarial learning for end-to-end Text-to-Speech**. That is a mouthful, so let me break it down.

<div class="plain-box">
**In plain English:** VITS is a type of AI model that turns text directly into a voice recording in one step. Most older TTS systems worked in two steps: first convert text to a picture of sound (called a spectrogram), then convert that picture into actual audio. VITS skips the middle step — you put text in, you get audio out.
</div>

### How VITS works internally

The model is made up of several components working in sequence:

**Step 1 — Text Encoder.**  
Reads the input text, tokenises it (breaks it into sound units), and converts it into a sequence of mathematical vectors. The encoder has 6 Transformer layers, each with a hidden size of 192 numbers, 2 attention heads, and a feed-forward dimension of 768.

<div class="plain-box">
**In plain English:** The text encoder reads the text and converts each character/sound into a list of numbers the model can work with. "Muraho" becomes something like [0.23, -0.14, 0.87, ...] for each part of the word.
</div>

**Step 2 — Duration Predictor.**  
Decides how long each sound unit should last. Uses a **stochastic flow-based model** (a type of probabilistic model that maps random noise into structured outputs) with 4 flows and 10 bins.

<div class="plain-box">
**In plain English:** This part decides the rhythm and pacing of the speech. Should this syllable be held longer? Should this word be faster? Without this, the voice would sound robotic and evenly-spaced.
</div>

**Step 3 — Normalising Flows (Prior Encoder).**  
A series of 4 reversible mathematical transformations that map the text representation into a probability distribution in "latent space" (a compressed mathematical representation of what the audio should sound like).

<div class="plain-box">
**In plain English:** This is the bridge between text and audio. It converts the text-based description of speech into a compressed internal code of what the audio should sound like — similar to how a zip file compresses information.
</div>

**Step 4 — HiFi-GAN Decoder.**  
Converts the latent representation back into actual audio samples at 16,000 samples per second (16 kHz). It uses transposed convolutions (upsampling layers) to expand the compressed code back into a full waveform.

<div class="plain-box">
**In plain English:** This is the "speaker" — it takes the compressed audio code and expands it back into actual sound samples you can play. It does this in four upsampling steps, each multiplying the length of the signal.
</div>

The four upsampling steps multiply the signal length:

| Step | Rate | Kernel size |
|---|---|---|
| 1 | ×8 | 16 |
| 2 | ×8 | 16 |
| 3 | ×2 | 4 |
| 4 | ×2 | 4 |

**Total upsampling factor: 8 × 8 × 2 × 2 = 256.** Starting from the latent frame rate (based on a hop length of 256 samples), the decoder produces audio at 16,000 Hz. 

**The stochastic noise parameters** — two numbers that control how much randomness is injected during synthesis:

- `noise_scale = 0.667` (the default) — controls noise in the main synthesis path
- `noise_scale_duration = 0.8` (the default) — controls noise in the duration predictor

<div class="plain-box">
**In plain English:** These two numbers control how "random" each synthesis is. Higher values = more natural-sounding but noisier. Lower values = cleaner but slightly more uniform. The defaults were causing a crackling, static sound in our case, which I fixed later (see Section 5).
</div>

### Key model numbers at a glance

| Parameter | Value | What it means |
|---|---|---|
| Vocabulary size | 44 tokens | The model knows 44 distinct sound units in Kinyarwanda |
| Hidden size | 192 | Each internal representation is a vector of 192 numbers |
| Sampling rate | 16,000 Hz | 16,000 audio samples per second |
| Number of mel bands | 80 | Used in the training loss |
| Spectrogram bins | 513 | Size of the FFT frequency bins |
| Number of speakers | 1 | Single averaged voice (no speaker switching) |

---

# Preparing the Data

## Step 1 — Loading the raw dataset

The dataset was stored as **Parquet files** on Hugging Face. Parquet is a columnar data format (like a very efficient spreadsheet) that can store binary audio data alongside text metadata.

<div class="plain-box">
**In plain English:** Parquet files are like compressed Excel files — they store the audio recordings, their transcriptions, and speaker labels all in one neat package. I had to unpack them into individual audio files first.
</div>

I wrote a Python script using `pyarrow` (a library for reading Parquet) and `soundfile` (a library for reading and writing audio files) to:

1. Read each row of the Parquet file
2. Decode the raw audio bytes into a waveform
3. Resample to **16,000 Hz mono** (the format the model expects)
4. Save each clip as an individual `.wav` file in `data/raw_loaded/wav/`
5. Save a **metadata CSV** with columns: `path`, `text`, `speaker_id`, `split`, `duration_sec`

## Step 2 — Auditing what I had

Before training I ran an audit to understand the data. Key findings:

- **Total duration**: ~14 hours across all splits
- **Three speakers** (labeled 0, 1, 2 in `speaker_id`)
- **Clip length**: varied from less than 1 second to over 15 seconds
- **Text length**: ranged from single words to multi-sentence passages

The audit output was saved to `reports/dataset_report.json`.

<div class="plain-box">
**In plain English:** Before cooking, you inventory the fridge. I wanted to know exactly what I had — how many clips, how long they were, and whether the data was balanced across speakers.
</div>

## Step 3 — Cleaning the data

Three cleaning operations were applied to every clip, in order.

### Silence trimming

I removed silence at the start and end of each recording. The threshold was **0.01** — any sample with an absolute value below 0.01 was considered silence. I kept only the segment from the first non-silent sample to the last non-silent sample.

<div class="plain-box">
**In plain English:** Many recordings start and end with a moment of silence before or after the speaker talks. I cut that silence off so the model doesn't spend time learning "how to be quiet."
</div>

### Loudness normalisation

Each clip was scaled to a consistent loudness level. The target was **−23 LUFS** (Loudness Units relative to Full Scale — the broadcast standard for voice content).

The formula in two steps:

**Step A — Compute the target RMS (Root Mean Square):**

$$\text{target\_RMS} = 10^{\frac{-23}{20}} \times 0.1 = 0.00708$$

<div class="plain-box">
**In plain English:** RMS is the average "energy" of the audio signal. We're calculating what that energy level should be for our target loudness (-23 LUFS).
</div>

**Step B — Compute the scale factor and apply it:**

$$\alpha = \frac{\text{target\_RMS}}{\text{RMS}(x)}, \quad \text{where } \text{RMS}(x) = \sqrt{\frac{1}{N}\sum_{i=1}^{N} x_i^2}$$

$$x_{\text{normalised}} = \text{clip}(\alpha \cdot x,\; -1,\; 1)$$

<div class="plain-box">
**In plain English:** If the recording is too quiet, multiply it up. If it's too loud, bring it down. Then clip anything that goes above 1.0 or below -1.0 (the maximum range of digital audio). This way every clip sounds equally loud to the model during training — the model doesn't confuse "quiet speaker" with "a different speaking style."
</div>

### Duration filter

Clips shorter than **2 seconds** or longer than **12 seconds** were discarded.

<div class="plain-box">
**In plain English:** Very short clips don't contain enough speech for the model to learn from. Very long clips eat too much memory on CPU and can make training unstable. The 2–12 second window kept the useful recordings while discarding the problematic ones.
</div>

---

# Training the Model

## Setup

Fine-tuning was done using:

- **PyTorch**: the most widely-used deep learning library (made by Meta)
- **Hugging Face `transformers`**: provides `VitsModel` and `AutoTokenizer` for easy model access
- **Config file**: `config/mms_tts.yaml` stores all hyperparameters so anyone can reproduce the exact training run

The configuration file contained:

```yaml
base_model:          "facebook/mms-tts-kin"
epochs:              5
batch_size:          8
max_train_samples:   280
learning_rate:       5.0e-6
resume_lr:           2.0e-6
resume_from:         "artifacts/final_model"
sample_rate:         16000
```

<div class="plain-box">
**In plain English:** I stored every setting that controls training in one file. This means if anyone wants to re-run my training, they just use the same file and get the same result. No guessing what settings were used.
</div>

## The loss functions — how the model knows it's improving

A **loss function** is a number that tells the model how wrong its output is. During training the model adjusts its internal settings (called **weights** or **parameters**) to make this number as small as possible. I used two loss functions added together.

### Loss 1 — Waveform Loss (L1 / Mean Absolute Error)

Let:

- $y$ = the real audio waveform from the dataset (what it *should* sound like)
- $\hat{y} = f_\theta(x)$ = the audio waveform the model generated from text ($f_\theta$ means "the model with its current parameters $\theta$")
- $\Omega$ = the overlapping portion of both signals (cropped to the shorter one)

$$\mathcal{L}_{\text{wav}} = \frac{1}{|\Omega|} \sum_{i \in \Omega} |\hat{y}_i - y_i|$$

<div class="plain-box">
**In plain English:** For each audio sample (remember, there are 16,000 per second), compare what the model produced to what the real recording sounds like. Add up all the differences and take the average. The smaller this number, the closer the model's output is to the real thing. I used absolute difference (not squared) because it doesn't over-punish large errors — which can happen when the model generates a sound that's slightly out of phase with the target.
</div>

### Loss 2 — Mel-Spectrogram Loss (Perceptual Loss)

The problem with pure waveform comparison is that a tiny time shift between the model output and the target produces a huge error — even if both *sound* identical to a human. To fix this, I added a second loss that compares the audio in the **frequency domain** instead of the sample-by-sample domain.

A **mel spectrogram** converts audio from a time-domain signal into a 2D picture showing how different frequency bands (on a mel scale that mimics human hearing) change over time.

<div class="plain-box">
**In plain English:** Instead of comparing the raw audio samples directly, I convert both signals into a "sound fingerprint" — a visual map of what frequencies are present at each moment in time (like a heat map of the sound). Comparing these fingerprints is much more robust, because two identical-sounding voices will have very similar fingerprints even if the raw samples are shifted by a few milliseconds.
</div>

The mel spectrogram parameters:

| Parameter | Value | What it does |
|---|---|---|
| n_fft | 1024 | Window size for the Fourier transform (frequency analysis) |
| hop_length | 256 | How many samples to move forward each time step |
| win_length | 1024 | How many samples to include in each analysis window |
| n_mels | 80 | Number of frequency bands (80 bands on the mel scale) |
| f_min | 0 Hz | Lowest frequency captured |
| f_max | 8,000 Hz | Highest frequency captured (upper limit of speech) |

After computing both spectrograms, I took the **logarithm** of each:

$$M_g = \log\!\left(\max\!\left(M(\hat{y}),\; \varepsilon\right)\right), \quad M_t = \log\!\left(\max\!\left(M(y),\; \varepsilon\right)\right), \quad \varepsilon = 10^{-5}$$

<div class="plain-box">
**In plain English:** The log is taken because human hearing is logarithmic — we perceive the difference between 1 dB and 2 dB the same way we perceive the difference between 10 dB and 20 dB. The tiny $\varepsilon = 0.00001$ prevents us from taking the log of zero (which would be negative infinity and crash the calculation).
</div>

Then the loss is the average absolute difference between the two log-mel spectrograms:

$$\mathcal{L}_{\text{mel}} = \frac{1}{F \times T} \sum_{f=1}^{F} \sum_{t=1}^{T} \left|M_g(f,t) - M_t(f,t)\right|$$

where $F = 80$ (frequency bands) and $T$ = number of time frames.

### The combined total loss

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{wav}} + 2.0 \times \mathcal{L}_{\text{mel}}$$

<div class="plain-box">
**In plain English:** I added the two losses together, giving the mel-spectrogram loss twice as much weight (multiplied by 2.0). Why? Because the mel loss is on a logarithmic scale and naturally produces smaller numbers — without the weight boost, the waveform loss would dominate and the model would focus more on sample-by-sample accuracy than on getting the overall sound right. The 2.0 factor balances their contribution.
</div>

## The optimiser — how the model updates itself

An **optimiser** is the algorithm that adjusts the model's internal weights to reduce the loss. I used **AdamW**, which stands for Adam (Adaptive Moment Estimation) with **decoupled Weight decay**.

<div class="plain-box">
**In plain English:** Every time the model generates audio, we measure how wrong it was (the loss). Then we nudge the model's millions of internal settings slightly in the direction that would have made it less wrong. The optimiser decides exactly how big that nudge should be and in which direction. Adam is the standard choice for fine-tuning neural networks because it adapts the nudge size individually for each setting.
</div>

**AdamW settings:**

| Setting | Value | Meaning |
|---|---|---|
| Learning rate ($\eta$) | $5 \times 10^{-6}$ | How big each update step is |
| Resume learning rate | $2 \times 10^{-6}$ | Smaller steps when continuing a previous run |
| Weight decay ($\lambda$) | 0.01 | Penalty for large weights (prevents overfitting) |
| $\beta_1$ | 0.9 | How much to remember from previous gradient directions |
| $\beta_2$ | 0.999 | How much to remember from previous gradient magnitudes |
| $\varepsilon$ | $10^{-8}$ | Tiny number to prevent division by zero |

The update formulas:

$$m_t = 0.9 \cdot m_{t-1} + 0.1 \cdot g_t \quad \text{(smoothed gradient direction)}$$
$$v_t = 0.999 \cdot v_{t-1} + 0.001 \cdot g_t^2 \quad \text{(smoothed gradient magnitude)}$$
$$\theta_t = \theta_{t-1} - \eta_t \left(\frac{m_t}{\sqrt{v_t} + \varepsilon} + \lambda \theta_{t-1}\right)$$

<div class="plain-box">
**In plain English:** $m_t$ is a running average of which direction the gradient has been pointing (to avoid random zigzag updates). $v_t$ is a running average of how big the gradient has been (to avoid too-large or too-small steps for each parameter). Together they give each weight its own personalised step size. The $\lambda \theta_{t-1}$ term gently pushes all weights toward zero each step — this is "weight decay" and it prevents the model from memorising the training data instead of learning general speech patterns.
</div>

## Learning rate schedule — cosine annealing

Rather than keeping the learning rate constant throughout training, I used a **cosine annealing schedule** that smoothly reduces the learning rate from its maximum to a minimum floor over the full training run.

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\!\left(\frac{\pi t}{T_{\text{total}}}\right)\right)$$

where $T_{\text{total}} = \text{epochs} \times \text{steps\_per\_epoch}$, $\eta_{\min} = 10^{-6}$, and $t$ is the current training step.

<div class="plain-box">
**In plain English:** Imagine learning to drive. At the start, you make big corrections (large learning rate). As you get closer to driving correctly, you make smaller and smaller adjustments (decreasing learning rate). The cosine curve is a smooth wave that does exactly this — no sudden jumps, just a gentle decline. This helps the model land in a stable, good solution rather than bouncing around near the end of training.
</div>

## Gradient clipping

After computing gradients, I applied **gradient clipping** with a maximum norm of 1.0:

$$\text{if } \|g\|_2 > 1.0 \text{, then } g \leftarrow \frac{g}{\|g\|_2}$$

<div class="plain-box">
**In plain English:** Sometimes a single bad batch can produce a very large gradient — essentially telling the model "make a HUGE change right now." This can destroy weeks of learning in one step (called "exploding gradients"). Gradient clipping puts a cap on how large the update can be, like a speed limiter on a car. No matter how bad a batch is, the model can only change by a limited amount.
</div>

## The CPU constraint — and how I worked around it

Training on CPU created a hard time problem:

| Scenario | Calculation | Total time |
|---|---|---|
| Time per batch (measured) | ~90 seconds | — |
| Full dataset: 3,700 samples ÷ batch 8 | 462 batches/epoch | 11.5 h/epoch |
| 50 epochs (typical for VITS) | 50 × 11.5 h | **~24 days** |

24 days was not feasible. I introduced `max_train_samples = 280`:

| With max_train_samples=280 | Calculation | Total time |
|---|---|---|
| 280 samples ÷ batch 8 | 35 batches/epoch | ~52 min/epoch |
| 5 epochs | 5 × 52 min | **~4–5 hours total** |

<div class="plain-box">
**In plain English:** With the full dataset, training would have taken nearly a month — clearly impossible. By using a random subset of 280 samples per epoch (changed each epoch), I got training down to 5 hours. The model sees a different random selection every epoch, so over 5 epochs it actually sees 1,400 unique sample-epoch combinations. This is a documented trade-off: less total exposure to data, but multiple passes make it meaningful. It is not ideal, but it is the best achievable without a GPU.
</div>

## Saving the model

At the end of each epoch, if the average loss was better than any previous epoch, the model was saved to `artifacts/final_model/` using `model.save_pretrained()`. This creates a Hugging Face–compatible directory with:

- `config.json` — all architectural settings
- `tokenizer_config.json`, `vocab.json`, `added_tokens.json` — the text tokeniser
- `model.safetensors` — the actual trained weights (~250 MB)

Every 5 epochs, an additional backup copy was saved to `artifacts/checkpoints/`.

---

# Making the Model Speak — The Inference Engine

## Loading the right model

The inference engine (`inference/tts_engine.py`) is the core Python module that runs during every synthesis. It starts by deciding which model to load:

1. Look in `artifacts/final_model/` for weight files (`model.safetensors` or `pytorch_model.bin`)
2. If weight files are found → load the fine-tuned model
3. If no weight files are found → download and use the pretrained `facebook/mms-tts-kin` from Hugging Face

<div class="plain-box">
**In plain English:** The engine always works — even if training hasn't happened yet. If you have a trained model, it uses that. If you don't, it falls back to the pretrained voice that already speaks Kinyarwanda.
</div>

## Fixing the static sound — lowering the noise scales

After loading the model, the first thing I do is adjust two parameters:

```
noise_scale:          0.667  →  0.333   (halved)
noise_scale_duration: 0.800  →  0.400   (halved)
```

<div class="plain-box">
**In plain English:** VITS deliberately injects a small amount of randomness (noise) into each synthesis to make the speech sound natural rather than mechanical. Think of it like a singer who doesn't hit exactly the same note every time — small variations make it sound human. But at the default settings, the amount of noise was too high for CPU-generated output, and it was coming through in the audio as a crackling, static sound — like an FM radio losing signal. By reducing the noise injection by half, the output became clean and clear while still sounding natural.
</div>

These values can be adjusted without changing code by setting environment variables:
`TTS_NOISE_SCALE=0.333` and `TTS_NOISE_SCALE_DURATION=0.4`.

## Cleaning the audio — post-processing

After the model generates the raw waveform, two operations are applied before the audio is served to the user.

### DC removal (high-pass filter)

A **1st-order Butterworth high-pass filter** at 60 Hz, applied with `filtfilt` (zero-phase filtering):

$$H(s) = \frac{s}{s + 2\pi \times 60}$$

<div class="plain-box">
**In plain English:** This is an extremely gentle filter that only removes the very lowest frequencies — essentially removing any DC offset (a flat, constant electrical drift below the audible range). It has no effect on the voice whatsoever. Think of it as removing the "hum" of electrical equipment from a microphone recording, while leaving the voice completely untouched.

**Why I replaced the old filter:** The original post-processing used a 4th-order bandpass filter (80 Hz–7,000 Hz). This was meant to "clean up" the audio but actually made things much worse. A 4th-order filter on a short VITS waveform creates a phenomenon called **ringing** — the filter resonates at its cutoff frequencies and adds its own artificial wavering sound to the signal. This was the cause of the "FM radio without connection" effect that was reported. I removed it entirely and replaced it with this much gentler alternative.
</div>

### Peak normalisation

$$a_{\text{out}} = \frac{a}{\max|a|} \times 0.95$$

<div class="plain-box">
**In plain English:** Find the loudest point in the audio. Scale the entire signal so that loudest point is at 95% of the maximum volume. This makes the output as loud as possible without clipping (distorting) — the 5% headroom is a safety buffer. Every synthesis comes out at a consistent, clear loudness level.
</div>

### Output format

16-bit PCM (the standard format for CD-quality audio), mono channel, 16,000 Hz. Written to an in-memory buffer and returned as raw bytes. No file is saved to disk; the audio goes directly to the browser or API caller.

## Latency

| Call type | Approximate time |
|---|---|
| Very first call (loads model from disk/downloads) | 30–90 seconds |
| All subsequent calls — short sentence (~10 words) | 400–800 ms ✓ |
| All subsequent calls — long sentence (20+ words) | 1,000–2,000 ms |

<div class="plain-box">
**In plain English:** The first call takes a long time because the model has to load into memory (like opening a large program for the first time). Every call after that is fast — well within the 800 ms target for short sentences. In production, the model loads once when the server starts, and every actual user request takes less than a second.
</div>

---

# The API

I built a **REST API** using R Plumber (`inference/api_plumber.R`). A REST API is like a menu at a restaurant — the caller says what they want, the kitchen (server) prepares it, and sends it back.

<div class="plain-box">
**In plain English:** The API lets any other system (a phone system, a web app, another server) request speech synthesis by sending a text message over the internet and getting an audio file back. No screen needed, no manual interaction — just machine-to-machine communication.
</div>

The API runs on port 8000 and has two endpoints:

### POST /synthesize — Returns JSON with audio in base64 format

**Request:**
```json
{ "text": "Muraho, nagufasha gute uyu munsi?", "speaker_id": null }
```

**Response:**
```json
{
  "wav_base64": "UklGRiQ...",
  "latency_ms": 523.4,
  "sampling_rate": 16000
}
```

<div class="plain-box">
**In plain English:** `wav_base64` is the audio file converted to text (Base64 encoding converts binary data to a text string so it can travel inside a JSON message). `latency_ms` is how many milliseconds the synthesis took. The caller can decode the base64 string back into an audio file and play it.
</div>

### POST /synthesize_wav — Returns raw audio directly

Same input, but the response is a raw `audio/wav` binary file — ready to play immediately.

---

# The Shiny Application

The Shiny app (`shiny_app/app.R`) is the user-facing demonstration interface. It has two tabs.

## Tab 1 — Writings

A full two-column technical write-up of this project, including all formulas, all decisions, and downloadable copies of the main scripts (`run_synthesis.py`, `requirements.txt`, `train_mms_tts.py`, `docker-compose.yml`, and others).

## Tab 2 — Model demonstration

- **Five required evaluation sentences** displayed as quick-click buttons
- A **text box** for typing any Kinyarwanda sentence
- A **speaker selector** (Default, Speaker 1, Speaker 2, Speaker 3)
- A **Generate speech** button that synthesises the audio, shows the latency in milliseconds, and displays a play button and a Download WAV link

## The tricky technical problem I solved in the app

When R loads a Python script using `reticulate::source_python()`, the Python functions are placed into a local environment. The original code then tried to read those functions from the global R environment using `sys.frame(0)` — which returned the old R placeholder instead of the Python function. This caused the app to always report "TTS not loaded" even when everything was set up correctly.

<div class="plain-box">
**In plain English:** Imagine putting a package in Room A (the local Python environment), but then going to look for it in Room B (the global R environment). You find the old empty box that was there before. I fixed this by creating a dedicated room (`tts_env`) specifically for the Python functions, and always reading from that exact room — never from anywhere else.
</div>

The fix:
```r
tts_env <- new.env(parent = emptyenv())
reticulate::source_python(tts_path, envir = tts_env)
py_synth <- tts_env$synthesize
```

---

# Docker — One Command to Run Everything

Docker packages the entire application — code, R, Python, and all libraries — into a portable container that runs the same way on any machine.

<div class="plain-box">
**In plain English:** Instead of sending someone instructions that say "install R, then install Python, then install 15 libraries, then configure these settings, then run this command," Docker packages everything into a box. You just run one command and the entire system starts up.
</div>

```bash
docker compose up
```

This single command starts two services:

| Service | Port | What it does |
|---|---|---|
| `inference` | 8000 | Runs the Plumber TTS API (R + Python via reticulate) |
| `shiny` | 3838 | Runs the Shiny demonstration app |

The `inference` service is built on `rocker/r-ver:4.3.2` (a minimal R environment), installs Python 3 and the required packages, and starts the Plumber API. The `shiny` service waits for the inference service to be ready before starting.

---

# Evaluation

## Subjective listening

The five required sentences were synthesised and are playable and downloadable directly in the Shiny app. Each was evaluated on three criteria:

1. **Intelligibility** — Can a Kinyarwanda speaker understand every word clearly?
2. **Naturalness** — Does the rhythm, stress, and intonation sound like a real speaker?
3. **Artefacts** — Is there static, buzzing, distortion, or any mechanical sound?

The five sentences:

> 1. *"Muraho, nagufasha gute uyu munsi?"* — Hello, how can I help you today?
> 2. *"Niba ufite ibibazo bijyanye n'ubuzima bwawe, twagufasha."* — If you have questions about your health, we can help.
> 3. *"Ni ngombwa ko ubonana umuganga vuba."* — It is important to see a doctor soon.
> 4. *"Twabanye nawe kandi tuzakomeza kukwitaho."* — We are with you and will continue to take care of you.
> 5. *"Ushobora kuduhamagara igihe cyose ukeneye ubufasha."* — You can call us any time you need help.

## Performance against constraints

| Constraint | Target | Result |
|---|---|---|
| Latency (short sentence) | < 800 ms | ✓ ~400–600 ms after first call |
| Model size | < 1 GB | ✓ ~250 MB |
| No external API | Required | ✓ All synthesis is local |
| Kinyarwanda support | Required | ✓ Native pretrained checkpoint |

---

# Four Problems I Hit — and How I Fixed Them

## Problem 1 — No model weights (silent crash)

**What happened:** The `artifacts/final_model/` directory existed and contained `config.json` (model architecture settings), but the actual weight file (`model.safetensors`) was missing because training hadn't been completed yet. The engine tried to load from this directory, failed silently, and the app crashed on first synthesis attempt.

**Root cause:** The check only looked for `config.json`, not for the actual weight files.

**Fix:** Added a `_has_weights()` function that specifically checks for `model.safetensors`, `pytorch_model.bin`, `tf_model.h5`, or `flax_model.msgpack`. Only if one of these exists does the engine use the local model. Otherwise it falls back to the pretrained Hugging Face model automatically.

<div class="plain-box">
**In plain English:** The code was checking if the folder existed (it did) and if the settings file existed (it did), but not checking if the actual brain of the model was in there. I added a check specifically for the brain file.
</div>

## Problem 2 — "FM radio without connection" audio quality

**What happened:** The synthesised speech had heavy crackling static noise on top of the voice — exactly like an FM radio losing signal.

**Root cause — two compounding issues:**

1. **`noise_scale = 0.667`** (the VITS default) was injecting too much stochastic noise into the synthesis path, audible as static on CPU-generated output
2. **A 4th-order Butterworth bandpass filter (80 Hz–7 kHz)** was being applied with `filtfilt` to the raw VITS output. Order-4 filters have steep cutoff slopes that create **ringing** — they add their own resonant oscillation to the signal at the cutoff frequencies

**Fix:**

1. Reduced `noise_scale` from 0.667 to **0.333** and `noise_scale_duration` from 0.8 to **0.4**
2. Removed the bandpass filter entirely
3. Replaced with a 1st-order high-pass at 60 Hz (removes only DC offset — has no effect on the voice)

<div class="plain-box">
**In plain English:** Two separate things were creating the noise. First, the model was told to "be random" at a level that was too high for CPU inference — like asking someone to add a pinch of salt and accidentally adding a cup. Second, a filter that was supposed to clean the audio was actually adding its own buzzing sound on top. I turned down the randomness and removed the bad filter.
</div>

## Problem 3 — `NameError: __file__ is not defined`

**What happened:** After synthesising, Python threw `NameError: name '__file__' is not defined`.

**Root cause:** `__file__` is a Python variable that stores the path to the currently running script file. When `reticulate::source_python()` runs Python code from R, it executes the code in an interpreter session with no associated script file — so `__file__` does not exist. The code tried to use `__file__` as a fallback to find the project root.

**Fix:** Wrapped the `__file__` usage in a `try/except NameError` block:

```python
try:
    root = Path(__file__).resolve().parents[1]
except NameError:
    root = Path.cwd()
```

In practice the fallback is never reached because `app.R` always sets `TTS_PROJECT_ROOT` before loading the engine.

<div class="plain-box">
**In plain English:** Python normally knows where its own script file is located (via `__file__`). But when another program (R) runs the Python code, there is no script file — so `__file__` doesn't exist. I wrapped that line in a try/catch so that if `__file__` is missing, the code simply uses the current working directory instead. Since the app always sets the project root via an environment variable anyway, this fallback is just a safety net.
</div>

## Problem 4 — `source_python` reading the wrong function

**What happened:** Even after everything was set up correctly, clicking "Generate speech" showed *"TTS not loaded. Install reticulate..."*

**Root cause:** `reticulate::source_python(tts_path)` places the Python `synthesize` function into its **local calling environment**. The original code then did:

```r
py_synth <- get("synthesize", envir = sys.frame(0))
```

`sys.frame(0)` is the **global R frame** — which already had an R placeholder `synthesize` function defined earlier. So `py_synth` captured the R placeholder (not the Python function), `tts_load_error` got cleared to `""`, and when synthesis was attempted, the placeholder ran and printed the generic error message.

**Fix:** Used a dedicated isolated environment:

```r
tts_env <- new.env(parent = emptyenv())
reticulate::source_python(tts_path, envir = tts_env)
py_synth <- tts_env$synthesize  # always the Python one
```

<div class="plain-box">
**In plain English:** Imagine you put a new item labelled "synthesize" (the Python function) in a shared drawer. But another item with the same label was already in that drawer (the R placeholder). When the code went to pick up "synthesize," it grabbed the old one. The fix: give the Python function its own private dedicated drawer so there is no name collision.
</div>

---

# The Complete Picture in One Table

| Step | Script | Technology | Key numbers / decisions |
|---|---|---|---|
| Load raw data | `01_load_dataset.R` + Python | pyarrow, soundfile | 14 h, 3 speakers, → 16 kHz mono WAV |
| Audit | `02_audit_report.R` | R | Duration, clip lengths, speaker distribution |
| Clean data | `03_clean_export.R` | R | Silence trim @0.01, normalise to −23 LUFS, keep 2–12 s clips |
| Run all steps | `run_pipeline.R` | R | One command; supports `--skip-if-exists` |
| Choose model | — | Research | MMS-TTS VITS over Piper/Coqui/from-scratch |
| Fine-tune | `train_mms_tts.py` | PyTorch | L_wav + 2.0×L_mel, AdamW lr=5e-6, cosine LR, grad clip 1.0 |
| Config | `mms_tts.yaml` | YAML | 5 epochs, batch 8, max_train_samples=280, ~5 h on CPU |
| Inference engine | `tts_engine.py` | Python | noise_scale=0.333, 60 Hz high-pass, peak norm ×0.95 |
| Simple CLI | `run_synthesis.py` | Python | `python inference/run_synthesis.py "text" out.wav` → prints latency |
| REST API | `api_plumber.R` | R Plumber | POST /synthesize (JSON) + POST /synthesize_wav (binary), port 8000 |
| Shiny app | `app.R` | R Shiny + reticulate | 2 tabs; runs model in-app; isolated tts_env fix |
| Docker | `docker-compose.yml` | Docker | `docker compose up` → inference:8000 + shiny:3838 |
| Deployed app | shinyapps.io | R Shiny | https://didier-ngamije.shinyapps.io/shiny_app/ |
| Code | GitHub | Git | https://github.com/ngamijex/TekanaAI |

---

# Use of Generative AI

Generative AI tools were used to assist with:

- Code structure and boilerplate for the R and Python scripts
- Writing and formatting documentation
- Suggesting CSS styles for the Shiny app

All of the following were done entirely by me:

- The decision to choose `facebook/mms-tts-kin` and the rejection of alternatives
- The choice of loss functions and the $\lambda_{\text{mel}} = 2.0$ weighting
- The learning rate ($5 \times 10^{-6}$), schedule (cosine), and `max_train_samples=280` strategy
- Diagnosing and fixing all four technical problems described above
- The noise scale reduction from 0.667 to 0.333 and the removal of the bandpass filter
- All evaluation and interpretation of results

---

*To regenerate this document: open `docs/technical_report.Rmd` in RStudio and click **Knit**, or run `rmarkdown::render("docs/technical_report.Rmd")` in the R console.*
