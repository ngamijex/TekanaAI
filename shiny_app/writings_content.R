# Detailed technical write-up for the Writings tab (sourced by app.R)
# Returns HTML string for the 2-column writeup.

writings_html_content <- function() {
  HTML(paste0(
    '<div class="writeup-header">',
    '<h2>Kinyarwanda Text-to-Speech: Methodology, Training, Testing, and Evaluation</h2>',
    '<p class="meta">Tekana Project — Data Scientist Take-Home Assessment (Technical Report)</p>',
    '</div>',
    '<div class="writeup-columns">',

    # 1. Introduction
    '<p><strong>1. Introduction and objective.</strong> This report describes the design, implementation, and evaluation of a Kinyarwanda text-to-speech (TTS) system for the Tekana voice-based AI. The goal is to convert the system’s text responses into natural, understandable speech under strict constraints: inference latency under 800 ms for short (≈10-word) sentences, model size ideally under 200 MB (hard limit 1 GB), and no storage or transmission of user voice data to external APIs. The work uses approximately 14 hours of confidential Kinyarwanda speech (pre-processed, with transcriptions and speaker labels) and follows a reproducible pipeline from data loading through training to inference and evaluation.</p>',

    # 2. Model choice and architecture
    '<p><strong>2. Model choice and architecture.</strong> We use <em>facebook/mms-tts-kin</em>, a VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model pre-trained for Kinyarwanda within the Massively Multilingual Speech (MMS) project. The dataset has three speakers; we fine-tune a single averaged voice (no speaker conditioning) so that one compact model serves all responses. Alternatives considered included Piper TTS, Coqui TTS, and training VITS from scratch; MMS-TTS was chosen for its existing Kinyarwanda checkpoint, efficient footprint, and Hugging Face integration. VITS combines a conditional variational autoencoder (cVAE) with normalising flows and adversarial training: the model maps text (via a text encoder and duration predictor) to a latent representation, then a decoder produces the waveform. The duration predictor uses flow-based modelling; the decoder uses transposed convolutions (upsample rates [8,8,2,2], kernel sizes [16,16,4,4]) to generate samples at 16 kHz. The internal representation is aligned with mel-spectrogram-like features (spectrogram bins 513, 80 mels used in our training loss). Model size and inference cost are compatible with the assignment constraints.</p>',

    # 3. Data preparation (detailed)
    '<p><strong>3. Data preparation.</strong> The dataset is provided as parquet files on Hugging Face, with columns <code>audio</code> (raw bytes), <code>txt</code> (transcription), <code>speaker_id</code>, and pre-defined train/validation/test splits. Step 1 (load): we decode audio to 16 kHz mono WAVs using a Python script (pyarrow + soundfile), write a metadata CSV (path, text, speaker_id, split, duration_sec). Step 2 (audit): we compute and report total duration, clip-length and text-length distributions per split. Step 3 (clean and export): we apply (a) <em>silence trimming</em> by thresholding sample magnitude (threshold 0.01), keeping the segment from the first to the last sample above threshold; (b) <em>loudness normalisation</em> to approximate −23 LUFS via RMS scaling: target RMS = 10^(target_LUFS/20) × 0.1, scale factor α = target_RMS / RMS(x), then clip to [−1, 1]; (c) <em>duration filter</em> keeping clips between 2 and 12 seconds. Processed WAVs and metadata are written to <code>data/processed/wav</code> and <code>data/processed/metadata.csv</code>. All steps are scripted in R and Python and orchestrated by <code>run_pipeline.R</code>.</p>',

    # 4. Methodology – loss functions
    '<p><strong>4. Methodology — loss functions.</strong> Fine-tuning minimises a weighted sum of two losses on each batch. Let x denote the input text (token ids), y the target waveform (float32, mono, 16 kHz), and ŷ = f_θ(x) the model output waveform (same length or longer; we crop to min length for loss). <em>Waveform loss:</em> L<sub>wav</sub> = (1/|Ω|) ∑<sub>i∈Ω</sub> |ŷ<sub>i</sub> − y<sub>i</sub>|, where Ω is the set of sample indices after cropping to min(length(ŷ), length(y)); this is the mean absolute error (L1). <em>Mel-spectrogram loss:</em> we compute 80-band mel spectrograms with n_fft = 1024, hop_length = 256, win_length = 1024, f_min = 0, f_max = 8000 Hz. Let M(·) be this transform; then M_g = log(max(M(ŷ), ε)), M_t = log(max(M(y), ε)) with ε = 10<sup>−5</sup>. We define L<sub>mel</sub> = (1/|F×T|) ∑<sub>f,t</sub> |M_g(f,t) − M_t(f,t)| over the mel dimensions (F = 80, T = number of frames). The <em>total loss</em> is L = L<sub>wav</sub> + λ_mel L<sub>mel</sub> with λ_mel = 2.0 so that the mel term (which is on log-mel scale and typically larger in magnitude) drives stable alignment and the waveform term refines the sample-level output.</p>',

    # 5. Training procedure
    '<p><strong>5. Training procedure.</strong> We use PyTorch with the Hugging Face <code>VitsModel</code> and <code>AutoTokenizer</code>. Optimiser: AdamW with learning rate η (5×10<sup>−6</sup> for initial training, 2×10<sup>−6</sup> when resuming from <code>artifacts/final_model</code>), weight decay 0.01. Update rule: m_t = β_1 m_{t−1} + (1−β_1)g_t, v_t = β_2 v_{t−1} + (1−β_2)g_t^2, θ_t = θ_{t−1} − η_t (m_t/(√v_t + ε) + λ θ_{t−1}) with β_1=0.9, β_2=0.999, ε=10<sup>−8</sup>. Learning-rate schedule: cosine annealing over the total number of steps T_total = epochs × steps_per_epoch, with minimum η_min = 10<sup>−6</sup>: η_t = η_min + (1/2)(η_max − η_min)(1 + cos(π t / T_total)). Gradients are clipped to maximum norm 1.0. Batches are formed from the training split (optionally capped by <code>max_train_samples</code>); each batch contains B sequences (B = 8 by default) with token ids padded to the same length and waveforms zero-padded to the longest clip in the batch. Batches with min(length(ŷ), length(y)) &lt; 1600 samples are skipped. The best model (by epoch-averaged L) is saved to <code>artifacts/final_model</code>; every 5 epochs a copy is saved to <code>artifacts/checkpoints</code>. Hyperparameters (epochs, batch_size, η, λ_mel, max_train_samples, resume_from, resume_lr) are read from <code>config/mms_tts.yaml</code>.</p>',

    # 6. CPU constraints and trade-offs
    '<p><strong>6. CPU-only training and trade-offs.</strong> Training was performed on CPU only. Measured time per batch is on the order of 80–100 s (hardware-dependent). With ~3700 training samples and batch size 8, one epoch is ~460 batches (≈10–12 h). Fifty full epochs would require hundreds of hours. To obtain a usable model within 4–12 h, we introduce an optional <code>max_train_samples</code> (e.g. 280 or 800). With 280 samples and B = 8, each epoch has 35 batches (~1 h per epoch); 5 epochs fit in ~5 h. This trades “full data, single epoch” for “subset, multiple epochs” so that the model still sees repeated passes and learns the target distribution. It is an explicit, documented trade-off under the constraint of no GPU.</p>',

    # 7. Testing during training
    '<p><strong>7. Testing during and after training.</strong> During training we do not run separate validation synthesis at each step; we rely on the training loss (L, L<sub>wav</sub>, L<sub>mel</sub>) logged every ~10% of steps per epoch and on the epoch-averaged loss to decide when to save the best model. After training, we test by running inference on fixed sentences (the five required evaluation sentences and any custom text) in the Shiny app or via the inference script. We measure end-to-end latency (time from synthesis call to WAV bytes) and listen for intelligibility, naturalness, and artefacts. No automated objective metrics (e.g. MOS, WER on ASR) are computed in the current pipeline; the evaluation is subjective and latency-based as specified.</p>',

    # 8. Evaluation (detailed)
    '<p><strong>8. Evaluation.</strong> <em>Subjective listening:</em> Evaluators listen to synthesised samples in the Shiny app (Model demonstration tab). The five required sentences are: “Muraho, nagufasha gute uyu munsi?”; “Niba ufite ibibazo bijyanye n\'ubuzima bwawe, twagufasha.”; “Ni ngombwa ko ubonana umuganga vuba.”; “Twabanye nawe kandi tuzakomeza kukwitaho.”; “Ushobora kuduhamagara igihe cyose ukeneye ubufasha.” Each can be synthesised, played, and downloaded as WAV. <em>Latency:</em> For each synthesis we record the wall-clock time from the start of the model forward pass to the availability of the WAV bytes; this is reported in milliseconds. The target is &lt;800 ms for short sentences; the first call after app start includes model load and can be higher (e.g. 30–90 s on CPU). <em>Model size:</em> The saved model (config, tokenizer, weights) is checked to remain within the 1 GB hard limit. <em>Deliverables:</em> This write-up, the codebase (data pipeline, training script, config, inference engine, Shiny app), requirements files, and the option to package the API (e.g. Docker) are provided.</p>',

    # 9. Inference post-processing (with formulas)
    '<p><strong>9. Inference and post-processing.</strong> At inference time we take text, run the model to get a float32 waveform at 16 kHz, then apply: (1) <em>Bandpass filter</em> (Butterworth, order 4, passband 80 Hz–7 kHz) to retain the speech band and attenuate rumble and hiss. The filter is applied in the time domain via forward–backward filtering (filtfilt) for zero phase shift. (2) <em>Peak normalisation:</em> a = a / max(|a|) × 0.95 so that the maximum absolute sample is 0.95, then clip to [−1, 1]. (3) <em>WAV export:</em> 16-bit PCM at the model sampling rate (16 kHz). No user audio is stored or sent to external APIs.</p>',

    # 10. Current behaviour and plans
    '<p><strong>10. Current model behaviour and plans.</strong> The current fine-tuned model produces audible Kinyarwanda speech but can exhibit residual noise and a slightly distant or muffled quality, consistent with limited training (subset, few epochs, CPU). Post-processing (bandpass, peak norm) improves perceived clarity. Planned improvements: (i) full-data, multi-epoch training on GPU when available; (ii) continued fine-tuning from the current checkpoint with <code>resume_from</code> and lower <code>resume_lr</code>; (iii) latency tuning on target hardware; (iv) optional speaker-conditioned generation.</p>',

    # 11. Accessing the model
    '<p><strong>11. Accessing the model (files and GitHub).</strong> The fine-tuned model is stored under <code>artifacts/final_model</code> (Hugging Face–style: <code>config.json</code>, tokenizer files, model weights). Checkpoints are in <code>artifacts/checkpoints</code>. To access via GitHub: clone the repository (<code>git clone https://github.com/&lt;your-username&gt;/TekanaAI</code>). If the model is too large for the repo, it may be provided as a GitHub Release asset or a separate link (see README). To use: set the working directory to the project root, install R packages (e.g. <code>reticulate</code>, <code>base64enc</code>) and ensure a Python environment with <code>transformers</code>, <code>torch</code>, <code>soundfile</code>, <code>scipy</code>; then run <code>shiny::runApp("shiny_app")</code>. The app loads the model from <code>artifacts/final_model</code> automatically. For script/API inference: <code>inference/tts_engine.py</code> with <code>TTS_PROJECT_ROOT</code> set to the project root, or <code>Rscript inference/run_plumber.R</code>.</p>',

    # 12. Code snippets and downloadable scripts
    '<p><strong>12. Code snippets and main script files.</strong> Below are example commands and the main scripts used in the pipeline. You can run the full pipeline from the project root, then train and run inference as follows. The listed script files are available for download at the end of this section.</p>',

    '<p><strong>Provided.</strong> This project provides the following (all downloadable from this page where applicable):</p>',
    '<ul class="writeup-scripts">',
    '<li><strong>Simple inference script</strong> — Takes a text string and outputs a WAV file; <em>prints latency (ms) for each call</em>. Download: <a href="scripts/run_synthesis.py" download="run_synthesis.py" class="script-download">run_synthesis.py</a>. Usage from project root: <code>python inference/run_synthesis.py "Muraho" output.wav</code></li>',
    '<li><strong>requirements.txt</strong> — Python dependencies for inference (transformers, torch, soundfile, scipy, etc.). Download: <a href="scripts/requirements.txt" download="requirements.txt" class="script-download">requirements.txt</a></li>',
    '<li><strong>Docker setup</strong> — Runs with a single command (<code>docker compose up</code> from project root) or exposes an API: inference service on port 8000, optional Shiny app on 3838. Download: <a href="scripts/docker-compose.yml" download="docker-compose.yml" class="script-download">docker-compose.yml</a> (place in project root; build context uses <code>docker/Dockerfile.inference</code> and <code>docker/Dockerfile.shiny</code>)</li>',
    '</ul>',

    '<div class="writeup-code-box"><span class="code-label">Simple inference (text → WAV; latency printed)</span><pre><code># From project root; latency printed for each call\npython inference/run_synthesis.py "Muraho, nagufasha gute?" out.wav</code></pre></div>',
    '<div class="writeup-code-box"><span class="code-label">Run full pipeline (data load, audit, clean)</span><pre><code># From project root\nRscript run_pipeline.R</code></pre></div>',
    '<div class="writeup-code-box"><span class="code-label">Train MMS-TTS (config from YAML)</span><pre><code>python scripts/training/train_mms_tts.py --config config/mms_tts.yaml</code></pre></div>',
    '<div class="writeup-code-box"><span class="code-label">Inference API (Plumber)</span><pre><code>Rscript inference/run_plumber.R   # API on http://0.0.0.0:8000</code></pre></div>',
    '<p><strong>Main script files (downloadable):</strong></p>',
    '<ul class="writeup-scripts">',
    '<li><a href="scripts/run_synthesis.py" download="run_synthesis.py" class="script-download">run_synthesis.py</a> — simple inference: text → WAV, prints latency</li>',
    '<li><a href="scripts/requirements.txt" download="requirements.txt" class="script-download">requirements.txt</a> — Python dependencies for inference</li>',
    '<li><a href="scripts/docker-compose.yml" download="docker-compose.yml" class="script-download">docker-compose.yml</a> — single-command Docker (inference + optional Shiny)</li>',
    '<li><a href="scripts/run_pipeline.R" download="run_pipeline.R" class="script-download">run_pipeline.R</a> — orchestrates data load, audit, and clean</li>',
    '<li><a href="scripts/mms_tts.yaml" download="mms_tts.yaml" class="script-download">mms_tts.yaml</a> — training config (epochs, batch size, LR, loss weight)</li>',
    '<li><a href="scripts/train_mms_tts.py" download="train_mms_tts.py" class="script-download">train_mms_tts.py</a> — fine-tuning script</li>',
    '<li><a href="scripts/tts_engine.py" download="tts_engine.py" class="script-download">tts_engine.py</a> — inference engine (synthesis + post-process)</li>',
    '<li><a href="scripts/run_plumber.R" download="run_plumber.R" class="script-download">run_plumber.R</a> — Plumber API for TTS</li>',
    '</ul>',

    '<p class="writeup-footer"><em>Generative AI tools were used for code structure and documentation assistance; model choice, data pipeline design, training configuration, loss formulation, and evaluation decisions are the author’s.</em></p>',

    '</div>'
  ))
}
